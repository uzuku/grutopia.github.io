# 30 minutes to get started

In this tutorial, we'll add a MyCobot280Pi to

## Get robot resources

### 1. URDF and usd

we can get full resources [here](https://github.com/elephantrobotics/mycobot_ros/tree/noetic/mycobot_description/urdf/mycobot_280_pi)

if you are not familiar to `ROS`/`ROS2`, you need to rename the source path in `URDF` file

![img.png](../_static/image/before_rename_urdf_img.png)

my resource path is `/home/apx103/Desktop/mycobot_280_pi`, so change meshes' `filename` attr like this:

```xml
<visual>
  <geometry>
   <mesh filename="file:///home/apx103/Desktop/mycobot_280_pi/joint4.dae"/>
  </geometry>
  <origin xyz = "0.0 0 0.03056 " rpy = " 0 -1.5708 0"/>
</visual>
```

### 2. import URDF and gen usd file

follow this [instruction](https://docs.omniverse.nvidia.com/isaacsim/latest/advanced_tutorials/tutorial_advanced_import_urdf.html) to get usd like this:

```log
.
├── materials
│   ├── joint2.png
│   ├── joint3.png
│   ├── joint4.png
│   ├── joint5.png
│   ├── joint6.png
│   └── joint7.png
└──  mycobot_280pi_with_camera_flange.usd
```

then follow this [instruction](https://docs.omniverse.nvidia.com/isaacsim/latest/advanced_tutorials/tutorial_motion_generation_robot_description_editor.html) to get robot description

my description is like this:

```yaml

# The robot description defines the generalized coordinates and how to map those
# to the underlying URDF dofs.

api_version: 1.0

# Defines the generalized coordinates. Each generalized coordinate is assumed
# to have an entry in the URDF.
# Lula will only use these joints to control the robot position.
cspace:
    - joint2_to_joint1
    - joint3_to_joint2
    - joint4_to_joint3
    - joint5_to_joint4
    - joint6_to_joint5
    - joint6output_to_joint6

root_link: g_base

default_q: [
    0.0,-0.0,-0.0,-0.0,0.0,-0.0
]

# Most dimensions of the cspace have a direct corresponding element
# in the URDF. This list of rules defines how unspecified coordinates
# should be extracted or how values in the URDF should be overwritten.

cspace_to_urdf_rules:

# Lula uses collision spheres to define the robot geometry in order to avoid
# collisions with external obstacles.  If no spheres are specified, Lula will
# not be able to avoid obstacles.

collision_spheres:
  - joint1:
    - "center": [0.0, 0.0, 0.039]
      "radius": 0.035
  - joint2:
    - "center": [0.0, 0.0, 0.0]
      "radius": 0.02
    - "center": [0.0, 0.0, -0.045]
      "radius": 0.02
    - "center": [0.0, 0.0, -0.011]
      "radius": 0.02
    - "center": [0.0, 0.0, -0.023]
      "radius": 0.02
    - "center": [0.0, 0.0, -0.034]
      "radius": 0.02
  - joint4:
    - "center": [0.0, 0.0, 0.0]
      "radius": 0.02
    - "center": [-0.094, -0.0, -0.0]
      "radius": 0.02
    - "center": [-0.016, -0.0, -0.0]
      "radius": 0.02
    - "center": [-0.031, -0.0, -0.0]
      "radius": 0.02
    - "center": [-0.047, -0.0, -0.0]
      "radius": 0.02
    - "center": [-0.063, -0.0, -0.0]
      "radius": 0.02
    - "center": [-0.078, -0.0, -0.0]
      "radius": 0.02
  - joint3:
    - "center": [-0.0, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.107, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.018, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.036, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.053, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.071, -0.0, 0.064]
      "radius": 0.02
    - "center": [-0.089, -0.0, 0.064]
      "radius": 0.02
  - joint5:
    - "center": [0.0, 0.0, 0.0]
      "radius": 0.02
  - joint6:
    - "center": [0.0, 0.0, 0.0]
      "radius": 0.02
```

### 3. Test files

> Test whether these files work

open `lula test extension` in isaac sim to test files we get before

![img.png](../_static/image/lula_test_extension.png)

we only need to test `lula Kunematics Solver`. And then we know:

1. robot(manipulator) can get solve using `lula Kunematics Solver`.
2. description file of robot(manipulator) is work.

## Add Robot

### What we need to do

In this example, we just add a IK controller to set the position(angle included) of manipulator's end effector.

### move resources to `assets`

we need to move resources to `assets/robots/mycobot_280_pi_with_camera_flange` like:

```log
.
├── materials
│   ├── joint2.png
│   ├── joint3.png
│   ├── joint4.png
│   ├── joint5.png
│   ├── joint6.png
│   └── joint7.png
├── mycobot_280pi_with_camera_flange.urdf
├── mycobot_280pi_with_camera_flange.usd
└── robot_descriptor.yaml
```

### Add Robot Class

1. At first, we need to define a Robot class based on `omni.isaac.core.robots.robot.Robot`. It wraps `end effector` and `articulation`.

    ```python
    import os
    import numpy as np
    from omni.isaac.core.prims import RigidPrim
    from omni.isaac.core.robots.robot import Robot as IsaacRobot
    from omni.isaac.core.scenes import Scene
    from omni.isaac.core.utils.nucleus import get_assets_root_path
    from omni.isaac.core.utils.stage import add_reference_to_stage

    from grutopia.core.config.robot import RobotUserConfig as Config
    from grutopia.core.robot.robot import BaseRobot
    from grutopia.core.robot.robot_model import RobotModel
    from grutopia.core.util import log



    class MyCobot280(IsaacRobot):
        def __init__(self,
                     prim_path: str,
                     usd_path: str,
                     name: str,
                     position: np.ndarray = None,
                     orientation: np.ndarray = None,
                     scale: np.ndarray = None):
            add_reference_to_stage(
                prim_path=prim_path, usd_path=os.path.abspath(usd_path))
            super().__init__(
                prim_path=prim_path,
                name=name,
                position=position,
                orientation=orientation,
                scale=scale)
            self._end_effector_prim_path = prim_path + '/joint6_flange'
            self._end_effector = RigidPrim(
                prim_path=self._end_effector_prim_path,
                name=name + '_end_effector',
            )

    @property
    def end_effector_prim_path(self):
        return self._end_effector_prim_path

    @property
    def end_effector(self):
        return self._end_effector
    ```

2. Then we need to wrap this Robot with `grutopia.core.robot.BaseRobot`

    ```python
    @BaseRobot.regester('MyCobot280PiRobot')
    class MyCobot280PiRobot(BaseRobot):
    def __init__(self, config: Config, robot_model: RobotModel, scene: Scene):
        super().__init__(config, robot_model, scene)
        self._sensor_config = robot_model.sensors
        self._start_position = np.array(
            config.position) if config.position is not None else None
        self._start_orientation = np.array(
            config.orientation) if config.orientation is not None else None

        log.debug(f'mycobot_280_pi {config.name}: position    : ' +
                  str(self._start_position))
        log.debug(f'mycobot_280_pi {config.name}: orientation : ' +
                  str(self._start_orientation))

        usd_path = robot_model.usd_path
        if usd_path.startswith('/Isaac'):
            usd_path = get_assets_root_path() + usd_path
        print(usd_path)

        log.debug(f'mycobot_280_pi {config.name}: usd_path         : ' +
                  str(usd_path))
        log.debug(f'mycobot_280_pi {config.name}: config.prim_path : ' +
                  str(config.prim_path))
        self.isaac_robot = MyCobot280(
            prim_path=config.prim_path,
            name=config.name,
            position=self._start_position,
            orientation=self._start_orientation,
            usd_path=usd_path,
        )

        self._robot_scale = np.array([1.0, 1.0, 1.0])
        if config.scale is not None:
            self._robot_scale = np.array(config.scale)
            self.isaac_robot.set_local_scale(self._robot_scale)

        self._robot_ik_base = RigidPrim(
            prim_path=config.prim_path + '/joint1',
            name=config.name + '_ik_base_link',
        )

        self._robot_base = RigidPrim(
            prim_path=config.prim_path + '/g_base',
            name=config.name + '_base_link')

    def get_robot_scale(self):
        return self._robot_scale

    def get_robot_ik_base(self):
        return self._robot_ik_base

    def get_world_pose(self):
        return self._robot_base.get_world_pose()

    def apply_action(self, action: dict):
        """
        Args:
            action (dict): inputs for controllers.
        """
        for controller_name, controller_action in action.items():
            if controller_name not in self.controllers:
                log.warn(f'unknown controller {controller_name} in action')
                continue
            controller = self.controllers[controller_name]
            control = controller.action_to_control(controller_action)
            self.isaac_robot.apply_action(control)

    def get_obs(self):
        """Add obs you need here."""
        position, orientation = self._robot_base.get_world_pose()

        # custom
        obs = {
            'position': position,
            'orientation': orientation,
        }

        eef_world_pose = self.isaac_robot.end_effector.get_world_pose()
        obs['eef_position'] = eef_world_pose[0]
        obs['eef_orientation'] = eef_world_pose[1]

        # common
        obs.update(super().get_obs())
        return obs
   ```
### Add Controller

In this example, we use a framework integrated controller `grutopia_extension.controllers.ik_controller` to solve IK for our robot.

```Python
# yapf: disable
from typing import Dict, List, Tuple

import numpy as np
from omni.isaac.core.articulations import Articulation
from omni.isaac.core.scenes import Scene
from omni.isaac.core.utils.numpy.rotations import rot_matrices_to_quats
from omni.isaac.core.utils.types import ArticulationAction
from omni.isaac.motion_generation import ArticulationKinematicsSolver, LulaKinematicsSolver

from grutopia.core.robot.controller import BaseController
from grutopia.core.robot.robot import BaseRobot
from grutopia.core.robot.robot_model import ControllerModel

# yapf: enable


@BaseController.register('InverseKinematicsController')
class InverseKinematicsController(BaseController):

    def __init__(self, config: ControllerModel, robot: BaseRobot, scene: Scene):
        super().__init__(config=config, robot=robot, scene=scene)
        self._kinematics_solver = KinematicsSolver(
            robot_articulation=robot.isaac_robot,
            robot_description_path=config.robot_description_path,
            robot_urdf_path=config.robot_urdf_path,
            end_effector_frame_name=config.end_effector_frame_name,
        )
        self.joint_subset = self._kinematics_solver.get_joints_subset()
        if config.reference:
            assert config.reference in ['world', 'robot', 'arm_base'], \
                f'unknown ik controller reference {config.reference}'
            self._reference = config.reference
        else:
            self._reference = 'world'

        self.success = False
        self.last_action = None
        self.threshold = 0.01 if config.threshold is None else config.threshold

        self._ik_base = robot.get_robot_ik_base()
        self._robot_scale = robot.get_robot_scale()
        if self._reference == 'robot':
            # The local pose of ik base is assumed not to change during simulation for ik controlled parts.
            # However, the world pose won't change even its base link has moved for some robots like ridgeback franka,
            # so the ik base pose returned by get_local_pose may change during simulation, which is unexpected.
            # So the initial local pose of ik base is saved at first and used during the whole simulation.
            self._ik_base_local_pose = self._ik_base.get_local_pose()

    def get_ik_base_world_pose(self) -> Tuple[np.ndarray, np.ndarray]:
        if self._reference == 'robot':
            ik_base_pose = self._ik_base_local_pose
        elif self._reference == 'arm_base':
            # Robot base is always at the origin.
            ik_base_pose = (np.array([0, 0, 0]), np.array([1, 0, 0, 0]))
        else:
            ik_base_pose = self._ik_base.get_world_pose()
        return ik_base_pose

    def forward(self, eef_target_position: np.ndarray,
                eef_target_orientation: np.ndarray) -> Tuple[ArticulationAction, bool]:
        self.last_action = [eef_target_position, eef_target_orientation]

        if eef_target_position is None:
            # Keep joint positions to lock pose.
            subset = self._kinematics_solver.get_joints_subset()
            return subset.make_articulation_action(joint_positions=subset.get_joint_positions(),
                                                   joint_velocities=subset.get_joint_velocities()), True

        ik_base_pose = self.get_ik_base_world_pose()
        self._kinematics_solver.set_robot_base_pose(robot_position=ik_base_pose[0] / self._robot_scale,
                                                    robot_orientation=ik_base_pose[1])
        return self._kinematics_solver.compute_inverse_kinematics(
            target_position=eef_target_position / self._robot_scale,
            target_orientation=eef_target_orientation,
        )

    def action_to_control(self, action: List | np.ndarray):
        """
        Args:
            action (np.ndarray): n-element 1d array containing:
              0. eef_target_position
              1. eef_target_orientation
        """
        assert len(action) == 2, 'action must contain 2 elements'
        assert self._kinematics_solver is not None, 'kinematics solver is not initialized'

        eef_target_position = None if action[0] is None else np.array(action[0])
        eef_target_orientation = None if action[1] is None else np.array(action[1])

        result, self.success = self.forward(
            eef_target_position=eef_target_position,
            eef_target_orientation=eef_target_orientation,
        )
        return result

    def get_obs(self) -> Dict[str, np.ndarray]:
        """Compute the pose of the robot end effector using the simulated robot's current joint positions

        Returns:
            Dict[str, np.ndarray]:
            - eef_position: eef position
            - eef_orientation: eef orientation quats
            - success: if solver converged successfully
            - finished: applied action has been finished
        """
        ik_base_pose = self.get_ik_base_world_pose()
        self._kinematics_solver.set_robot_base_pose(robot_position=ik_base_pose[0] / self._robot_scale,
                                                    robot_orientation=ik_base_pose[1])
        pos, ori = self._kinematics_solver.compute_end_effector_pose()

        finished = False
        if self.last_action is not None:
            if self.last_action[0] is not None:
                dist_from_goal = np.linalg.norm(pos - self.last_action[0])
                if dist_from_goal < self.threshold * self.robot.get_robot_scale()[0]:
                    finished = True

        return {
            'eef_position': pos * self._robot_scale,
            'eef_orientation': rot_matrices_to_quats(ori),
            'success': self.success,
            'finished': finished,
        }


class KinematicsSolver(ArticulationKinematicsSolver):
    """Kinematics Solver for robot.  This class loads a LulaKinematicsSovler object

    Args:
        robot_description_path (str): path to a robot description yaml file \
            describing the cspace of the robot and other relevant parameters
        robot_urdf_path (str): path to a URDF file describing the robot
        end_effector_frame_name (str): The name of the end effector.
    """

    def __init__(self, robot_articulation: Articulation, robot_description_path: str, robot_urdf_path: str,
                 end_effector_frame_name: str):
        self._kinematics = LulaKinematicsSolver(robot_description_path, robot_urdf_path)

        ArticulationKinematicsSolver.__init__(self, robot_articulation, self._kinematics, end_effector_frame_name)

        if hasattr(self._kinematics, 'set_max_iterations'):
            self._kinematics.set_max_iterations(150)
        else:
            self._kinematics.ccd_max_iterations = 150

        return

    def set_robot_base_pose(self, robot_position: np.array, robot_orientation: np.array):
        return self._kinematics.set_robot_base_pose(robot_position=robot_position, robot_orientation=robot_orientation)

```

### Add config

After add robot and add controller, we need register our robot to `grutopia_extension/robots/robot_models.yaml` as follows

```yaml
- type: "MyCobot280PiRobot"
  usd_path: "GRUtopia/assets/robots/mycobot_280pi_with_camera_flange/mycobot_280pi_with_camera_flange.usd"
  controllers:
  - name: "ik_controller"
    type: "InverseKinematicsController"
    robot_description_path: "GRUtopia/assets/robots/mycobot_280pi_with_camera_flange/robot_descriptor.yaml"
    robot_urdf_path: "GRUtopia/assets/robots/mycobot_280pi_with_camera_flange/mycobot_280pi_with_camera_flange.urdf"
    end_effector_frame_name: "joint6_flange"
    threshold: 0.01
```
this file combine robot with controllers and sensors, and setup some param that app users have not deed to know.

## Test

to test a new robot in our framework. we need to create two files:

1. run file: define running process.
2. config file: define scene\robots\objects and any well load to isaac sim dynamically.

### `run file`

```python
from grutopia.core.config import SimulatorConfig
from grutopia.core.env import BaseEnv

file_path = './GRUtopia/demo/configs/follow_target_mycobot.yaml'
sim_config = SimulatorConfig(file_path)

# env = BaseEnv(sim_config)
env = BaseEnv(sim_config, headless=False)

while env.simulation_app.is_running():
    actions = []
    for task in env.config.tasks:
        target_cube_pose = env.runner.get_obj(task.objects[0].name).get_world_pose()
        action = {
            'mycobot': {
                'ik_controller': target_cube_pose,
            },
        }
        actions.append(action)
    observations = env.step(actions)

env.simulation_app.close()
```

### `config`

```yaml
simulator:
  physics_dt: 0.01666  # 1 / 60
  rendering_dt: 0.01666  # 1 / 60

env:
  bg_type: null

render:
  render: true

tasks:
- type: "SingleInferenceTask"
  name: "mycobot280pi_follow_cube"
  env_num: 1
  offset_size: 1.0
  robots:
  - name: mycobot
    prim_path: "/mycobot"
    type: "MyCobot280PiRobot"
    position: [.0, .0, .0]
    scale: [1, 1, 1]
    controller_params:
    - name: "ik_controller"
    - name: "rmp_controller"

  objects:
  - name: target_cube
    type: VisualCube
    prim_path: /target_cube
    position: [0.08, 0.1, 0.3]
    scale: [0.05015, 0.05015, 0.05015]
    color: [0, 0, 1.0]
```

### Run Test

Run test file at root path of isaac sim like:

```shell
python ./GRUtopia/demo/follow_target_mycobot280.py
```

we get a follow target mycobot demo

![img.png](../_static/image/follow_target_mycobot_demo.png)
